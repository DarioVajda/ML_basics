{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generating Image Captions\n",
        "\n",
        "This is a project that combines a CNN model with an LSTM model for generating image captions.\n",
        "A pretrained CNN is fine-tuned for feature extraction from the given image and the features are passes as input into the LSTM which generates the captions.\n",
        "\n",
        "\n",
        "\"model_name...\" (url...) was used as the pretrained CNN\n",
        "The \"name...\" dataset was used for the combined models, it can be found here ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install torch torchvision pandas nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOcrR2PHw3lq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choosing the device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJq1EIl6-pyd"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the captions from the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Paths to your dataset\n",
        "image_folder = './flickr30k_images/flickr30k_images'\n",
        "captions_file = './flickr30k_images/results.csv'\n",
        "\n",
        "# Load captions\n",
        "captions = pd.read_csv(captions_file, sep='\\t', header=None, names=['image', 'caption'])\n",
        "captions['image'] = captions['image'].apply(lambda x: x.split('#')[0])\n",
        "\n",
        "\n",
        "def split_string(input_str):\n",
        "    parts = input_str.split('|')\n",
        "    parts = [part.strip() for part in parts]\n",
        "    parts[1] = int(parts[1])\n",
        "    return parts\n",
        "\n",
        "# Display an example\n",
        "captions = captions['image'][1:].apply(split_string)\n",
        "\n",
        "\n",
        "# Clean and tokenize captions\n",
        "def preprocess_caption(element):\n",
        "    caption = element[2]\n",
        "    \n",
        "    caption = caption.lower()\n",
        "    caption = re.sub(r'[^\\w\\s]', '', caption)  # Remove punctuation\n",
        "\n",
        "    element.append(word_tokenize(caption) + ['<EOS>'])  # emp is used to pad the caption to a fixed length with empty space\n",
        "\n",
        "    return element\n",
        "\n",
        "captions_tokenized = captions.apply(preprocess_caption)\n",
        "# print(captions_tokenized)\n",
        "\n",
        "# Build vocabulary\n",
        "# all_words = [word for tokens in captions['tokens'] for word in tokens]\n",
        "# vocab = Counter(all_words)\n",
        "# vocab_size = len(vocab)\n",
        "\n",
        "# convert vocab to a python vocabulary with keys as image names and values as list of captions\n",
        "vocab_dict = {}\n",
        "for image_name, _, _, caption_tokens in captions_tokenized:\n",
        "    image_name = image_name.replace('.jpg', '')\n",
        "    if image_name not in vocab_dict:\n",
        "        vocab_dict[image_name] = []\n",
        "    vocab_dict[image_name].append(caption_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checking if the captins were loaded well:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if '1000268201' in vocab_dict:\n",
        "    print(vocab_dict['1000268201'])\n",
        "else:\n",
        "    print('Not found')\n",
        "if '4153903524' in vocab_dict:\n",
        "    print(vocab_dict['4153903524'][0])\n",
        "    print(vocab_dict['4153903524'][1])\n",
        "    print(vocab_dict['4153903524'])\n",
        "else:\n",
        "    print('Not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating the vocabulary based on all the tokenized captions\n",
        "\n",
        "all_words = [word for item in captions_tokenized for word in item[3]]\n",
        "\n",
        "vocab_counter = Counter(all_words)\n",
        "vocab_size = len(vocab_counter)\n",
        "\n",
        "# Print the results\n",
        "# print(all_words[0:100])\n",
        "# print(vocab_counter)\n",
        "\n",
        "# Get the most common tokens (sorted by frequency)\n",
        "most_common_tokens = vocab_counter.most_common()\n",
        "\n",
        "# Create the vocabulary dictionary\n",
        "vocab = {token: idx for idx, (token, _) in enumerate(most_common_tokens)}\n",
        "# print('vocab: ', vocab)\n",
        "\n",
        "def sentence_to_tensor(sentence):\n",
        "    return torch.tensor([vocab[token] for token in sentence])\n",
        "\n",
        "max_len = 0\n",
        "longest = 0\n",
        "total = 0\n",
        "longer_tnan_20 = 0\n",
        "for i in range(158915):\n",
        "    if i == 0: continue\n",
        "    total += len(captions_tokenized[i][3])\n",
        "    if len(captions_tokenized[i][3]) > max_len:\n",
        "        longest = captions_tokenized[i]\n",
        "        max_len = len(captions_tokenized[i][3])\n",
        "\n",
        "    if len(captions_tokenized[i][3]) > 20:\n",
        "        longer_tnan_20 += 1\n",
        "\n",
        "print('max_len: ', max_len)\n",
        "print('longest: ', longest[3])\n",
        "print('average: ', total/158914)\n",
        "print('longer_than_20: ', longer_tnan_20)\n",
        "# print('example tokens:   ', captions_tokenized[2][3])\n",
        "# print('example tensor: ', sentence_to_tensor(captions_tokenized[2][3]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function to get list of captions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_captions(image_name):\n",
        "    if image_name not in vocab_dict:\n",
        "        return None\n",
        "    return vocab_dict[image_name]\n",
        "\n",
        "print(get_captions('4153903524'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the images from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, image_folder, transform=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.image_files = [f for f in os.listdir(image_folder) if f.endswith('.jpg')]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_folder, img_name)\n",
        "        image = Image.open(img_path)\n",
        "        \n",
        "        # Extract label from the filename (remove extension)\n",
        "        label = os.path.splitext(img_name)[0]\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, label\n",
        "\n",
        "# Example usage:\n",
        "image_folder = './flickr30k_images/flickr30k_images'\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = CustomImageDataset(image_folder=image_folder, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Example loop through the dataloader\n",
        "# counter = 0\n",
        "# for images, labels in dataloader:\n",
        "#     if counter > 5:\n",
        "#         break\n",
        "#     counter += 1\n",
        "#     print(labels)  # this will print the image labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CNN model (from ResNet18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pre-trained ResNet18 model\n",
        "cnn_model = models.resnet18(pretrained=True)\n",
        "\n",
        "CNN_OUTPUT_SIZE = 256\n",
        "\n",
        "count = 0\n",
        "for param in cnn_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the last layer with a new, untrained layer\n",
        "num_features = cnn_model.fc.in_features # the number of features in the last layer of the model (512 for ResNet18)\n",
        "cnn_model.fc = nn.Linear(num_features, CNN_OUTPUT_SIZE)\n",
        "\n",
        "# Set the last layer to be trainable\n",
        "for param in cnn_model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "cnn_model = cnn_model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LSTM model implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper function for the LSTM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def output_to_token(output):\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    return predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q410fTFtF2r6"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, memory_size, output_size, max_output_len):\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        self.max_output_len = max_output_len\n",
        "        self.input_size = input_size\n",
        "        self.memory_size = memory_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, input_size)\n",
        "\n",
        "        # self.output_to_input = nn.Linear(output_size, input_size) # this is used for all but the first step\n",
        "\n",
        "        self.forget_x = nn.Linear(input_size, memory_size) # this is used for the % of the memory to remember\n",
        "        self.forget_h = nn.Linear(memory_size, memory_size) # -||-\n",
        "\n",
        "        self.input_x = nn.Linear(input_size, memory_size) # this is used for the % of a new memory to remember\n",
        "        self.input_h = nn.Linear(memory_size, memory_size) # -||-\n",
        "\n",
        "        self.output_x = nn.Linear(input_size, memory_size) # this is used to generate the output of the cell\n",
        "        self.output_h = nn.Linear(memory_size, memory_size) # -||-\n",
        "\n",
        "        self.new_memory_x = nn.Linear(input_size, memory_size) # this is used to generate the new memory\n",
        "        self.new_memory_h = nn.Linear(memory_size, memory_size) # -||-\n",
        "\n",
        "        self.output_layer = nn.Linear(memory_size, output_size) # this is used to generate the final output\n",
        "\n",
        "        # Learnable initial hidden and cell states\n",
        "        self.h_0 = nn.Parameter(torch.zeros(1, memory_size))\n",
        "        self.c_0 = nn.Parameter(torch.zeros(1, memory_size))\n",
        "\n",
        "    def forward_step(self, input_tensor, prev_hidden, prev_state):\n",
        "        # implement the logic for a single forward pass step for the LSTM (the formulas from torch documentation)\n",
        "        i = self.sigmoid(self.input_x(input_tensor) + self.input_h(prev_hidden))\n",
        "        f = self.sigmoid(self.forget_x(input_tensor) + self.forget_h(prev_hidden))\n",
        "        o = self.sigmoid(self.output_x(input_tensor) + self.output_h(prev_hidden))\n",
        "        g = self.tanh(self.new_memory_x(input_tensor) + self.new_memory_h(prev_hidden))\n",
        "\n",
        "        c = f * prev_state + i * g    # internal state\n",
        "        new_hidden = o * self.tanh(c) # short term memory\n",
        "        return new_hidden, c\n",
        "    \n",
        "    def forward(self, initial_input, ground_truth=None, teacher_forcing_ratio=0.5):\n",
        "        # working with the initial_input\n",
        "        batch_size = initial_input.size(0)\n",
        "        hidden = self.h_0.repeat(batch_size, 1).to(device)\n",
        "        state = self.c_0.repeat(batch_size, 1).to(device)\n",
        "\n",
        "        # capturing the outputs in a list\n",
        "        outputs = torch.zeros(self.max_output_len, batch_size, self.output_size).to(device)\n",
        "\n",
        "        # performing the first forward step\n",
        "        hidden, state = self.forward_step(initial_input, hidden, state)\n",
        "        outputs[0] = self.output_layer(hidden)\n",
        "\n",
        "        # performing the rest of the forward steps\n",
        "        for i in range(1, self.max_output_len):\n",
        "            # if teacher forcing is used, the next input is the ground truth\n",
        "            if ground_truth is not None and random.random() < teacher_forcing_ratio:\n",
        "                # setting the input tensor to the ground truth for the previous step output\n",
        "                prev_output = ground_truth[:, i-1]\n",
        "            else:\n",
        "                # getting the previous output and using it as the new input\n",
        "                prev_output = outputs[i-1].clone()\n",
        "\n",
        "            _, indices = prev_output.max(dim=1)\n",
        "            # print('indices: ', indices, \"; i = \", i)\n",
        "            input_tensor = self.embedding(indices)\n",
        "\n",
        "            # calling the forward step\n",
        "            hidden, state = self.forward_step(input_tensor, hidden, state)\n",
        "            outputs[i] = self.output_layer(hidden)\n",
        "        \n",
        "        return outputs.transpose(0, 1)\n",
        "\n",
        "MAX_OUTPUT_LEN = 20\n",
        "lstm_model = LSTM(CNN_OUTPUT_SIZE, 512, vocab_size, MAX_OUTPUT_LEN).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combining the CNN and LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CaptionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CaptionModel, self).__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.lstm_model = lstm_model\n",
        "\n",
        "    def forward(self, image_tensor, ground_truth=None, teacher_forcing_ratio=0.5):\n",
        "        image_features = self.cnn_model(image_tensor)\n",
        "        captions = self.lstm_model(image_features, ground_truth, teacher_forcing_ratio)\n",
        "        return captions\n",
        "\n",
        "caption_model = CaptionModel().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def indices_to_tokens(output):\n",
        "    sentence = []\n",
        "    for batch in output:\n",
        "        batch_sentence = []\n",
        "        for tokens in batch:\n",
        "            _, token_index = torch.max(tokens, dim=0)\n",
        "            batch_sentence.append(list(vocab.keys())[token_index])\n",
        "            # if token_index == vocab['<EOS>']:\n",
        "            #     break\n",
        "        sentence.append(batch_sentence)\n",
        "    return sentence\n",
        "\n",
        "def tokens_to_indices(sentence):\n",
        "    indices = [vocab[token] for token in sentence]\n",
        "    if len(indices) > MAX_OUTPUT_LEN:\n",
        "        indices = indices[:MAX_OUTPUT_LEN]\n",
        "    elif len(indices) < MAX_OUTPUT_LEN:\n",
        "        indices += [vocab['<EOS>']] * (MAX_OUTPUT_LEN - len(indices))\n",
        "    return torch.tensor(indices)\n",
        "\n",
        "def indexes_to_onehot(indexes):\n",
        "    # indexes: (batch_size, seq_len)\n",
        "    onehot = torch.zeros(indexes.size(0), indexes.size(1), len(vocab)).to(device)\n",
        "    onehot.scatter_(2, indexes.unsqueeze(2), 1)\n",
        "    return onehot\n",
        "\n",
        "def calculate_teacher_forcing_ratio(epoch, max_epochs):\n",
        "    ratio = 1 - epoch / max_epochs\n",
        "    return 0.1 + ratio*0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loss and Criterion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "loss_weights = torch.ones(len(vocab)).to(device)\n",
        "loss_weights[0] = 30 # reducing the importance of the token 'a'\n",
        "loss_weights[1] = 30 # reducing the importance of the <EOS> token\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(\n",
        "    # ignore_index=vocab['<EOS>'], \n",
        "    # reduction='mean',\n",
        "    # weight=loss_weights\n",
        ")\n",
        "# criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "optimizer = optim.SGD(caption_model.parameters(), lr=0.1, momentum=0.9)\n",
        "optimizer = optim.Adam(caption_model.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=900, gamma=0.966)\n",
        "\n",
        "# Example usage:\n",
        "# for i in range(5*990):\n",
        "#     scheduler.step()\n",
        "#     print(optimizer.state_dict()['param_groups'][0]['lr'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Some tests before training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "\n",
        "# The batch will contain both inputs (images) and labels\n",
        "inputs, labels = first_batch\n",
        "\n",
        "# You can now work with 'inputs' and 'labels'\n",
        "print(\"First batch inputs shape:\", inputs.shape)\n",
        "print(\"First batch labels:\", labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# this is how to get the captions for given file batch with names in the list 'labels'\n",
        "captions = [get_captions(f'{label}') for label in labels]\n",
        "print(captions)\n",
        "print(len(captions), len(captions[0]))\n",
        "\n",
        "shortest_captions = [min(captions[i], key=len) for i in range(len(captions))]\n",
        "shortest_indices = [captions[i].index(shortest_captions[i]) for i in range(len(captions))]\n",
        "print('shortest_indices: ', shortest_indices)\n",
        "\n",
        "target_output = []\n",
        "for i, caption_list in enumerate(captions):\n",
        "    target_output.append(tokens_to_indices(caption_list[shortest_indices[i]]))\n",
        "\n",
        "target_output = torch.stack(target_output).to(device)\n",
        "print('target_output.shape: ', target_output.shape)\n",
        "\n",
        "onehot_target = indexes_to_onehot(target_output)\n",
        "print('onehot_target.shape: ', onehot_target.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# testing the forward pass\n",
        "inputs = inputs.to(device)\n",
        "outputs = caption_model(inputs, onehot_target, 0.5)\n",
        "# print(outputs.shape)\n",
        "# print(outputs)\n",
        "\n",
        "print('outputs shape', outputs.shape)\n",
        "print('target output shape: ', target_output.shape)\n",
        "\n",
        "\n",
        "# print(BATCH_SIZE * MAX_OUTPUT_LEN, vocab_size)\n",
        "outputs_flat = outputs.reshape(BATCH_SIZE * MAX_OUTPUT_LEN, vocab_size)\n",
        "target_output_flat = target_output.reshape(BATCH_SIZE * MAX_OUTPUT_LEN)\n",
        "\n",
        "print(\"shapes: \", outputs_flat.shape, target_output_flat.shape)\n",
        "\n",
        "loss = criterion(outputs_flat, target_output_flat)\n",
        "print(loss.item())\n",
        "\n",
        "# print(indices_to_tokens(outputs))\n",
        "\n",
        "# loss = criterion(outputs, labels)\n",
        "# loss.backward()\n",
        "# optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "accumulation_steps = 10\n",
        "total_loss = 0\n",
        "\n",
        "checkpoint_path = 'checkpoint_with_embeddings.pth'\n",
        "\n",
        "# torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    for i, (inputs, labels) in enumerate(dataloader):\n",
        "        if epoch==0 and i==0 and os.path.exists(checkpoint_path):\n",
        "            # loading the state of the model from the checkpoint\n",
        "            print('loading the states from the checkpoint')\n",
        "\n",
        "            checkpoint = torch.load(checkpoint_path)\n",
        "            caption_model.load_state_dict(checkpoint['model_state'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
        "            scheduler.load_state_dict(checkpoint['scheduler_state'])\n",
        "            epoch = checkpoint['epoch']\n",
        "\n",
        "        # print(i)\n",
        "        if i == 990:\n",
        "            # making a checkpoint of the current model state\n",
        "            checkpoint = {\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state\": caption_model.state_dict(),\n",
        "                \"optimizer_state\": optimizer.state_dict(),\n",
        "                \"scheduler_state\": scheduler.state_dict()\n",
        "            }\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            break\n",
        "    \n",
        "        # here we get all 5 captions for the images in the batch\n",
        "        captions = [get_captions(f'{label}') for label in labels]\n",
        "\n",
        "        # here we get the indices of the tokens in the captions\n",
        "        target_output = []\n",
        "        shortest_captions = [min(captions[i], key=len) for i in range(len(captions))]\n",
        "        shortest_indices = [captions[i].index(shortest_captions[i]) for i in range(len(captions))]\n",
        "        for j, caption_list in enumerate(captions):\n",
        "            target_output.append(tokens_to_indices(caption_list[shortest_indices[j]]))\n",
        "        target_output = torch.stack(target_output).to(device)\n",
        "\n",
        "        # forward pass\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = caption_model(\n",
        "            inputs,                                         # image features tensor\n",
        "            indexes_to_onehot(target_output),               # target output tensor (onehot encoded tokens)\n",
        "            calculate_teacher_forcing_ratio(epoch, epochs)  # teacher forcing ratio (calculated based on the epoch)\n",
        "            # 1\n",
        "        )\n",
        "\n",
        "        # flattening the outputs and target_output tensors\n",
        "        outputs_flat = outputs.reshape(BATCH_SIZE * MAX_OUTPUT_LEN, vocab_size)\n",
        "        target_output_flat = target_output.reshape(BATCH_SIZE * MAX_OUTPUT_LEN)\n",
        "\n",
        "        # calculate loss\n",
        "        loss = criterion(outputs_flat, target_output_flat)\n",
        "\n",
        "        # accumulate gradients\n",
        "        loss = loss / accumulation_steps\n",
        "        loss.backward()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (i+1) % accumulation_steps == 0:\n",
        "            # update weights\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            scheduler.step()\n",
        "            print(f'Epoch {epoch + 1}, batch {i + 1}, loss: {total_loss}')\n",
        "            # print(f'Example image: {labels[0]} --> Caption: \"{output_to_token(outputs[1])}\"')\n",
        "            total_loss = 0.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "checkpoint_path = 'checkpoint_with_embeddngs.pth'\n",
        "\n",
        "# Set the device\n",
        "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the image folder and transform\n",
        "image_folder = './flickr30k_images/flickr30k_images'\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the trained model\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "caption_model.load_state_dict(checkpoint['model_state'])\n",
        "caption_model.to(device)\n",
        "caption_model.eval()\n",
        "\n",
        "# Get a random image from the dataset\n",
        "random_image = random.choice(dataset.image_files)\n",
        "image_path = os.path.join(image_folder, random_image)\n",
        "image = Image.open(image_path)\n",
        "print(image_path)\n",
        "\n",
        "# Preprocess the image\n",
        "image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "# Pass the image through the model\n",
        "with torch.no_grad():\n",
        "    cnn_outputs = caption_model.cnn_model(image_tensor)\n",
        "    outputs = caption_model(image_tensor)\n",
        "\n",
        "print(cnn_outputs[0][0:20])\n",
        "\n",
        "# Convert the output to tokens\n",
        "output_tokens = indices_to_tokens(outputs)\n",
        "# Format the output as a sentence\n",
        "output_sentence = ' '.join(output_tokens[0])\n",
        "\n",
        "# Print the output sentence\n",
        "print(output_sentence)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

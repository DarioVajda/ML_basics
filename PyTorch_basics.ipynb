{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0Fhn6eHtXwO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Torch Tensors"
      ],
      "metadata": {
        "id": "oqlADEQgtaOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making an empty tensor\n",
        "x = torch.empty(2, 3, 4)\n",
        "print(x)\n",
        "\n",
        "\n",
        "# making a tensor of zeros\n",
        "x = torch.zeros(2, 3, 4)\n",
        "print(x)\n",
        "\n",
        "\n",
        "# making a tensor of ones\n",
        "x = torch.ones(2, 3, 4, dtype=torch.int) # dtype changes the type of the values in the tensor\n",
        "print(x)\n",
        "\n",
        "\n",
        "# making a tensor with random values\n",
        "x = torch.rand(2, 2) # uniform distribution on [0, 1)\n",
        "x = torch.randn(2, 2) # normal distribution\n",
        "\n",
        "\n",
        "# operations with tensors\n",
        "x = torch.rand(2, 2)\n",
        "y = torch.rand(2, 2)\n",
        "print(x, y)\n",
        "z = x + y\n",
        "z = torch.add(x, y) # this is the same as z = x+y\n",
        "y.add_(x) # this is the same as y = y + x\n",
        "z.add_(1) # 1 is added to every element of z\n",
        "# everything is same with -, * and / (sub, mul, div)\n",
        "\n",
        "\n",
        "# slicing tensors\n",
        "x = torch.rand(5, 3)\n",
        "print(x[:, 0]) # 0-th column\n",
        "print(x[0, 0].item()) # value of the element in place [0, 0]\n",
        "\n",
        "\n",
        "# reshaping tensors\n",
        "x = torch.rand(4, 4)\n",
        "print(x.view(16)) # \"flattening\" the tensor\n",
        "print(x.view(-1, 8)) # making a ?x8 tensor ==> ? is 2\n",
        "\n",
        "\n",
        "# numpy <--- torch\n",
        "a = torch.ones(5)\n",
        "b = a.numpy() # array b will share memory locations with the tensor a, it only has the reference\n",
        "print(type(b))\n",
        "\n",
        "\n",
        "# numpy <--- torch\n",
        "a = np.ones(5)\n",
        "b = torch.from_numpy(a) # again only the reference is passed\n",
        "\n",
        "\n",
        "# Tensors on the GPU\n",
        "if(torch.cuda.is_available()):\n",
        "    device = torch.device(\"cuda\")\n",
        "    x = torch.ones(5, device=device)\n",
        "    y = torch.ones(5)\n",
        "    y = y.to(device)\n",
        "    # Note - can't convert between numpy and torch with tensors on GPU\n",
        "    y = y.to(\"cpu\") # moving the tensor back to the CPU\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "p5SzQ3dftn5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Torch Autograd"
      ],
      "metadata": {
        "id": "K7gJjPwlyFvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(3, requires_grad=True) # a computational grapht will be created so that later the gradient can be determined for variables dependant on x\n",
        "print(x)\n",
        "\n",
        "y = x * 2 + 2\n",
        "print(y)\n",
        "z = y * y\n",
        "z = z.mean() # calculating the mean of the elements in the tensor\n",
        "print(z)\n",
        "\n",
        "z.backward() # calculates the gradient dz/dx\n",
        "    # NOTE - .backward() method can be called if z is a scalar\n",
        "print(x.grad)\n",
        "\n",
        "\n",
        "# IMPORTANT - when updating the parameters of a NN, we don't want the gradient to be calculated\n",
        "# There are three soutions:\n",
        "x.requires_grad_(False)\n",
        "x.detach()\n",
        "with torch.no_grad():\n",
        "    y = x * 2 + 1\n",
        "    print(y)\n"
      ],
      "metadata": {
        "id": "90yj7xS4w8cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.randn(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "     output = (weights*3).sum()\n",
        "\n",
        "     output.backward()\n",
        "     print(weights.grad)\n",
        "\n",
        "     weights.grad.zero_() # reseting the gradients between training epochs"
      ],
      "metadata": {
        "id": "hXcBkoVi2Mrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USING OPTIMIZ ERS\n",
        "\n",
        "optimizer = torch.optim.SGD(weights, lr=0.01)\n",
        "\n",
        "output.backward()\n",
        "\n",
        "optimizer.step()\n",
        "optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "vDAZK6dC24Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backpropagation"
      ],
      "metadata": {
        "id": "hpqq6Rr_6_TW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "# forward pass and computing the loss\n",
        "y_hat = w * x\n",
        "loss = (y_hat - y)**2\n",
        "\n",
        "print(loss)\n",
        "\n",
        "# backward pass\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n",
        "# update the weights\n",
        "# ..."
      ],
      "metadata": {
        "id": "7kzagGtu7DVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient descent"
      ],
      "metadata": {
        "id": "EOQOenRA8slg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version #1\n",
        "\n",
        "*   prediction - manually\n",
        "*   gradient computation - manually\n",
        "*   loss computation - manually\n",
        "*   parameter updates - manually\n",
        "\n"
      ],
      "metadata": {
        "id": "fEl82spK82i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# only using numpy\n",
        "\n",
        "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
        "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
        "\n",
        "w = 0.0\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "# loss\n",
        "def loss(y, y_predicted):\n",
        "    return ((y_predicted-y)**2).mean()\n",
        "\n",
        "# gradient\n",
        "def gradient(x, y, y_predicted):\n",
        "    return np.dot(2*x, y_predicted - y).mean()\n",
        "\n",
        "\n",
        "print(f'Prediction before training f(5): {forward(5):.3f}')\n",
        "\n",
        "\n",
        "\n",
        "# Training\n",
        "lr = 0.01\n",
        "n_iters = 50\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    dw = gradient(X, Y, y_pred)\n",
        "\n",
        "    w -= lr * dw\n",
        "    if epoch % 5 == 0:\n",
        "        print(f'epoch: {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training f(5): {forward(5):.3f}')\n"
      ],
      "metadata": {
        "id": "HMLNhEJK8v9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version #2\n",
        "\n",
        "*   prediction - manually\n",
        "*   gradient computation - **Autograd**\n",
        "*   loss computation - manually\n",
        "*   parameter updates - manually"
      ],
      "metadata": {
        "id": "UI3s_zNy9M-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# only using numpy\n",
        "\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "# loss\n",
        "def loss(y, y_predicted):\n",
        "    return ((y_predicted-y)**2).mean()\n",
        "\n",
        "\n",
        "print(f'Prediction before training f(5): {forward(5):.3f}')\n",
        "\n",
        "\n",
        "\n",
        "# Training\n",
        "lr = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    l.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        w -= lr * w.grad\n",
        "\n",
        "    w.grad.zero_()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'epoch: {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training f(5): {forward(5):.3f}')"
      ],
      "metadata": {
        "id": "eDO5uyKn9RZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version #3\n",
        "\n",
        "*   prediction - manually\n",
        "*   gradient computation - **Autograd**\n",
        "*   loss computation - **PyTorch Loss**\n",
        "*   parameter updates - **Pytorch Optimizer**"
      ],
      "metadata": {
        "id": "rDWZ4xzS9RmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# only using numpy\n",
        "\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "print(f'Prediction before training f(5): {forward(5):.3f}')\n",
        "\n",
        "\n",
        "# Training\n",
        "lr = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD([w], lr)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    l.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'epoch: {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training f(5): {forward(5):.3f}')"
      ],
      "metadata": {
        "id": "hiY2x4s79ZIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version #4\n",
        "\n",
        "*   prediction - **Pytoch Model**\n",
        "*   gradient computation - **Autograd**\n",
        "*   loss computation - **PyTorch Loss**\n",
        "*   parameter updates - **Pytorch Optimizer**"
      ],
      "metadata": {
        "id": "RdzyKV969gxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# only using numpy\n",
        "\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
        "\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "\n",
        "# model = nn.Linear(n_features, n_features)\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearRegression, self).__init__()\n",
        "        self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin(x)\n",
        "\n",
        "model = LinearRegression(n_features, n_features)\n",
        "\n",
        "\n",
        "print(f'Prediction before training f(5): {model(X_test).item():.3f}')\n",
        "\n",
        "\n",
        "# Training\n",
        "lr = 0.05\n",
        "n_iters = 500\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    y_pred = model(X)\n",
        "\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    l.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        [w, b] = model.parameters()\n",
        "        print(f'epoch: {epoch + 1}: w = {w[0, 0].item() :.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training f(5): {model(X_test).item():.3f}')"
      ],
      "metadata": {
        "id": "fOZcA-Aw9lsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "V-QyWXn-fgqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the datasets"
      ],
      "metadata": {
        "id": "ScpvuVgzfu4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "9UuQ2x2KfkWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 0 - Preparing the data:"
      ],
      "metadata": {
        "id": "O3uc_NlCfyme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# scaling the features\n",
        "sc =  StandardScaler() # sc stands for standard scaler\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "y_train = y_train.view(y_train.shape[0], 1)\n",
        "y_test = y_test.view(y_test.shape[0], 1)"
      ],
      "metadata": {
        "id": "q8hU138vgEr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 - Model"
      ],
      "metadata": {
        "id": "GqBOHpyhioBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, n_input_features):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.lin = nn.Linear(n_input_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_pred = torch.sigmoid(self.lin(x))\n",
        "        return y_pred\n",
        "\n",
        "model = LogisticRegression(n_features)"
      ],
      "metadata": {
        "id": "0fCumxKBiudv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Digression*\n",
        "\n",
        "Step 1.5 - Model with Activation Functions\n",
        "\n",
        "- Here is the list of possible activation function: ReLU, Sigmoid, Softmax, TanH, LeakyReLU (they are all accessed by nn.* or torch.*)"
      ],
      "metadata": {
        "id": "vuXzQm1fsyo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression2(nn.Module):\n",
        "    def __init__(self, n_input_features):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.lin1 = nn.Linear(n_input_features, 5)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.lin2 = nn.Linear(5, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.lin2(relu)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "model = LogisticRegression(n_features)"
      ],
      "metadata": {
        "id": "CC3No6jgs5rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2 - Loss and Optimizer"
      ],
      "metadata": {
        "id": "a99RWyVyjrLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "IxmvViX4juM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3 - Training Loop"
      ],
      "metadata": {
        "id": "OIoFjoU0kBwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    y_pred = model(X_train)\n",
        "    loss = criterion(y_pred, y_train)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (epoch+1) % 1000 == 0:\n",
        "        print(f'epoch: {epoch+1}, loss={loss.item():.4f}')"
      ],
      "metadata": {
        "id": "8YxAnX_dkF_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation:"
      ],
      "metadata": {
        "id": "avcE7uZpkyo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    y_pred = model(X_test)\n",
        "    y_pred_cls = y_pred.round()\n",
        "    acc = y_pred_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
        "    print(f'acc={acc:.4f}')"
      ],
      "metadata": {
        "id": "BriTVyUgk1Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets and DataLoaders"
      ],
      "metadata": {
        "id": "z_9lD6QUrSaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ZhvCwFJZrWWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uploading the file to google colab\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "y7SN_NxSt1DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WineDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # data loading\n",
        "        df = pd.read_csv('wine.csv', delimiter=\",\", dtype=np.float32, skiprows=1)\n",
        "        xy = df.to_numpy()\n",
        "        self.x = torch.from_numpy(xy[:, 1:])\n",
        "        self.y = torch.from_numpy(xy[:, [0]])\n",
        "        self.n_samples = xy.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index], self.y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "dataset = WineDataset()\n",
        "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "TWwPuaassCDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 2\n",
        "total_samples = len(dataset)\n",
        "n_iterations = math.ceil(total_samples/4)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (inputs, labels) in enumerate(dataloader):\n",
        "        # forward\n",
        "        # backward\n",
        "        # update\n",
        "        if (i+1)%5==0:\n",
        "            print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_iterations}, inputs {inputs.shape}')\n",
        "\n"
      ],
      "metadata": {
        "id": "2KuOSWsa5eba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Transforms"
      ],
      "metadata": {
        "id": "jVAxpSNGvzJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  See the list of built-in transforms: https://pytorch.org/vision/0.9/transforms.html"
      ],
      "metadata": {
        "id": "Q1FyUqXIv20m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making a dataset for testing purposes:"
      ],
      "metadata": {
        "id": "KmlLuHy5x5ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WineDatasetWithTransform(Dataset):\n",
        "    def __init__(self, transform=None):\n",
        "        # data loading\n",
        "        df = pd.read_csv('wine.csv', delimiter=\",\", dtype=np.float32, skiprows=1)\n",
        "        xy = df.to_numpy()\n",
        "        self.x = xy[:, 1:]\n",
        "        self.y = xy[:, [0]]\n",
        "        self.n_samples = xy.shape[0]\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.x[index], self.y[index]\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples"
      ],
      "metadata": {
        "id": "aZwr3qkrxGpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making a custom transforms:"
      ],
      "metadata": {
        "id": "_7t42c--xKG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ToTensor:\n",
        "    def __call__(self, sample):\n",
        "        inputs, targets = sample\n",
        "        return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
        "\n",
        "class MulTransform:\n",
        "    def __init__(self, factor):\n",
        "        self.factor = factor\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        inputs, target = sample\n",
        "        inputs *= self.factor\n",
        "        return inputs, target\n",
        "\n",
        "\n",
        "# dataset = WineDatasetWithTransform(transform=ToTensor())\n",
        "# first_data = dataset[0]\n",
        "# features, labels = first_data\n",
        "# print(type(features), type(labels))"
      ],
      "metadata": {
        "id": "bq6lcuf5v12-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Composing multiple transforms:"
      ],
      "metadata": {
        "id": "sW8obNoxxMca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying only one transform (for comparison):\n",
        "dataset = WineDatasetWithTransform(transform=ToTensor())\n",
        "first_data = dataset[0]\n",
        "features, labels = first_data\n",
        "print(\"Without MulTransform:\")\n",
        "print(features)\n",
        "print(type(features), type(labels))\n",
        "\n",
        "\n",
        "print(\"__________________________________\")\n",
        "\n",
        "# Applying a composition of more transforms:\n",
        "composed = torchvision.transforms.Compose([ ToTensor(), MulTransform(2) ])\n",
        "\n",
        "dataset = WineDatasetWithTransform(transform=composed)\n",
        "first_data = dataset[0]\n",
        "features, labels = first_data\n",
        "print(\"WITH MulTransform:\")\n",
        "print(features)\n",
        "print(type(features), type(labels))"
      ],
      "metadata": {
        "id": "RBJdhUxl0VBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Softmax and Cross Entropy Loss"
      ],
      "metadata": {
        "id": "L1bqgUYZCe7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Softmax** formula:\n",
        "\n",
        "$S(y_i)=\\frac{e^{y_i}}{\\sum_{j=0}^{n} e^{y_j}}$"
      ],
      "metadata": {
        "id": "IZ_VzmhM1Yfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# basic implementation of the softmax function\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "x = np.array([2.0, 1.0, 0.1])\n",
        "outputs = softmax(x)\n",
        "print(\"softmax numpy: \", outputs)\n",
        "\n",
        "# Torch has the softmax function implemented already\n",
        "x = torch.tensor([2.0, 1.0, 0.1])\n",
        "outputs = torch.softmax(x, dim=0)\n",
        "print(\"softmax torch: \", outputs)"
      ],
      "metadata": {
        "id": "_u6LaOBnCgqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross Entropy** formula:\n",
        "\n",
        "$D(\\hat{Y}, Y) = -\\frac{1}{N} \\cdot \\sum_i Y_i \\cdot \\log(\\hat{Y}_i)$\n",
        "\n",
        "- $Y$ - correct (expected) output\n",
        "- $\\hat{Y}$ - prediction for the output\n",
        "\n",
        "This is used to measure the performance of a classification model\n",
        "\n",
        "Note - Y must be hot encoded"
      ],
      "metadata": {
        "id": "QrdD7DSi8lYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# basic implementation of the scross entropy function\n",
        "def cross_entropy(y, y_hat):\n",
        "    return -np.sum(y * np.log(y_hat))\n",
        "\n",
        "Y = np.array([1, 0, 0])\n",
        "\n",
        "Y_pred_good = np.array([0.7, 0.2, 0.1])\n",
        "Y_pred_bad = np.array([0.1, 0.3, 0.6])\n",
        "\n",
        "l1 = cross_entropy(Y, Y_pred_good)\n",
        "l2 = cross_entropy(Y, Y_pred_bad)\n",
        "\n",
        "print(f'Loss1 numpy: {l1:.4f}')\n",
        "print(f'Loss2 numpy: {l2:.4f}')"
      ],
      "metadata": {
        "id": "ll5_w0jw-dkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using torch for the sross entropy function\n",
        "# advantage to using this is that it can work with more samples\n",
        "\n",
        "# No Softmax in the last layer ==> Y_pred has raw scores\n",
        "# Y <=> class labels, not one-hot\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "Y = torch.tensor([0, 1, 2, 0]) # correct answer is class 0\n",
        "\n",
        "# dimension is n_samples x n_classes ---> 4x3\n",
        "Y_pred_good = torch.tensor([[ 2.0, 1.0, 0.1 ], [ 0.8, 1.5, 0.2 ], [ 0.2, 0.7, 2.4 ], [ 2.0, 1.0, 0.1 ]])\n",
        "Y_pred_bad = torch.tensor([[ 0.5, 2.5, 0.3 ], [ 2.5, 0.3, 0.1 ], [ 0.5, 2.5, 0.3 ], [ 0.5, 2.5, 0.3 ]])\n",
        "\n",
        "# Using the nn.CrosEntropyLoss to calculate the losses\n",
        "l1 = criterion(Y_pred_good, Y)\n",
        "l2 = criterion(Y_pred_bad, Y)\n",
        "\n",
        "print(l1.item()) # calling .item() because this is a tensor with only one element\n",
        "print(l2.item())\n",
        "\n",
        "max1, predictions1 = torch.max(Y_pred_good, 1)\n",
        "max2, predictions2 = torch.max(Y_pred_bad, 1)\n",
        "print(max1, predictions1)\n",
        "print(max2, predictions2)"
      ],
      "metadata": {
        "id": "kOYSJNiSOZnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adjusting the learning rate"
      ],
      "metadata": {
        "id": "QoZjRCJ1NxEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "lr = 0.1\n",
        "model = nn.Linear(10, 1)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "lambda1 = lambda epoch: epoch / 10\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lambda1)\n",
        "\n",
        "print(optimizer.state_dict())\n",
        "\n",
        "for epoch in range(5):\n",
        "    # loss.backward()\n",
        "    optimizer.step()\n",
        "    # validate(...)\n",
        "    scheduler.step()\n",
        "    print(optimizer.state_dict()['param_groups'][0]['lr'])"
      ],
      "metadata": {
        "id": "LP00AEUvOQrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **List of schedulers:**\n",
        "- **LambdaLR**         --> initial_lr * function_result\n",
        "- **MultiplicativeLR** --> prev_lr * function_result\n",
        "- **StepLR**           --> prev_lr * *gamma* (every *step_size* iterations)\n",
        "- **MultiStepLR**      --> prev_lr * *gamma* (at every *milestones* iteration)\n",
        "- **ExponentialLR**    --> prev_lr * *gamma*\n",
        "- **ReducedLROnPlateau** --> ...\n",
        "- CyclicRL, OneCycleLR,..."
      ],
      "metadata": {
        "id": "mUS4soWiXNma"
      }
    }
  ]
}